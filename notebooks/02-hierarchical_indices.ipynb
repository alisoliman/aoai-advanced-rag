{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Indices in Document Retrieval\n",
    "\n",
    "## Overview\n",
    "\n",
    "This code implements a Hierarchical Indexing system for document retrieval, utilizing two levels of encoding: document-level summaries and detailed chunks. This approach aims to improve the efficiency and relevance of information retrieval by first identifying relevant document sections through summaries, then drilling down to specific details within those sections.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Traditional flat indexing methods can struggle with large documents or corpus, potentially missing context or returning irrelevant information. Hierarchical indexing addresses this by creating a two-tier search system, allowing for more efficient and context-aware retrieval.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. PDF processing and text chunking\n",
    "2. Asynchronous document summarization using OpenAI's GPT-4\n",
    "3. Vector store creation for both summaries and detailed chunks using FAISS and OpenAI embeddings\n",
    "4. Custom hierarchical retrieval function\n",
    "\n",
    "## Method Details\n",
    "\n",
    "### Document Preprocessing and Encoding\n",
    "\n",
    "1. The PDF is loaded and split into documents (likely by page).\n",
    "2. Each document is summarized asynchronously using GPT-4.\n",
    "3. The original documents are also split into smaller, detailed chunks.\n",
    "4. Two separate vector stores are created:\n",
    "   - One for document-level summaries\n",
    "   - One for detailed chunks\n",
    "\n",
    "### Asynchronous Processing and Rate Limiting\n",
    "\n",
    "1. The code uses asynchronous programming (asyncio) to improve efficiency.\n",
    "2. Implements batching and exponential backoff to handle API rate limits.\n",
    "\n",
    "### Hierarchical Retrieval\n",
    "\n",
    "The `retrieve_hierarchical` function implements the two-tier search:\n",
    "\n",
    "1. It first searches the summary vector store to identify relevant document sections.\n",
    "2. For each relevant summary, it then searches the detailed chunk vector store, filtering by the corresponding page number.\n",
    "3. This approach ensures that detailed information is retrieved only from the most relevant document sections.\n",
    "\n",
    "## Benefits of this Approach\n",
    "\n",
    "1. Improved Retrieval Efficiency: By first searching summaries, the system can quickly identify relevant document sections without processing all detailed chunks.\n",
    "2. Better Context Preservation: The hierarchical approach helps maintain the broader context of retrieved information.\n",
    "3. Scalability: This method is particularly beneficial for large documents or corpus, where flat searching might be inefficient or miss important context.\n",
    "4. Flexibility: The system allows for adjusting the number of summaries and chunks retrieved, enabling fine-tuning for different use cases.\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "1. Asynchronous Programming: Utilizes Python's asyncio for efficient I/O operations and API calls.\n",
    "2. Rate Limit Handling: Implements batching and exponential backoff to manage API rate limits effectively.\n",
    "3. Persistent Storage: Saves the generated vector stores locally to avoid unnecessary recomputation.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Hierarchical indexing represents a sophisticated approach to document retrieval, particularly suitable for large or complex document sets. By leveraging both high-level summaries and detailed chunks, it offers a balance between broad context understanding and specific information retrieval. This method has potential applications in various fields requiring efficient and context-aware information retrieval, such as legal document analysis, academic research, or large-scale content management systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/hierarchical_indices.svg\" alt=\"hierarchical_indices\" style=\"width:50%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/hierarchical_indices_example.svg\" alt=\"hierarchical_indices\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas pyPDF2 langchain_openai langchain langchain-community faiss-cpu python-dotenv pyarrow fastparquet huggingface_hub ipywidgets pypdf ragas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from openai import RateLimitError\n",
    "\n",
    "\n",
    "async def exponential_backoff(attempt):\n",
    "    \"\"\"\n",
    "    Implements exponential backoff with a jitter.\n",
    "    \n",
    "    Args:\n",
    "        attempt: The current retry attempt number.\n",
    "        \n",
    "    Waits for a period of time before retrying the operation.\n",
    "    The wait time is calculated as (2^attempt) + a random fraction of a second.\n",
    "    \"\"\"\n",
    "    # Calculate the wait time with exponential backoff and jitter\n",
    "    wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "    print(f\"Rate limit hit. Retrying in {wait_time:.2f} seconds...\")\n",
    "\n",
    "    # Asynchronously sleep for the calculated wait time\n",
    "    await asyncio.sleep(wait_time)\n",
    "\n",
    "\n",
    "async def retry_with_exponential_backoff(coroutine, max_retries=5):\n",
    "    \"\"\"\n",
    "    Retries a coroutine using exponential backoff upon encountering a RateLimitError.\n",
    "    \n",
    "    Args:\n",
    "        coroutine: The coroutine to be executed.\n",
    "        max_retries: The maximum number of retry attempts.\n",
    "        \n",
    "    Returns:\n",
    "        The result of the coroutine if successful.\n",
    "        \n",
    "    Raises:\n",
    "        The last encountered exception if all retry attempts fail.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Attempt to execute the coroutine\n",
    "            return await coroutine\n",
    "        except RateLimitError as e:\n",
    "            # If the last attempt also fails, raise the exception\n",
    "            if attempt == max_retries - 1:\n",
    "                raise e\n",
    "\n",
    "            # Wait for an exponential backoff period before retrying\n",
    "            await exponential_backoff(attempt)\n",
    "\n",
    "    # If max retries are reached without success, raise an exception\n",
    "    raise Exception(\"Max retries reached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import getpass\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.chains.summarize.chain import load_summarize_chain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n",
    "\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "api_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT') \n",
    "api_key=os.getenv('AZURE_OPENAI_API_KEY')\n",
    "llm_deployment_name = os.getenv('AZURE_OPENAI_MODEL_NAME')\n",
    "embedding_deployment_name = os.getenv('AZURE_OPENAI_EMBEDDING_MODEL')\n",
    "api_version = '2024-02-15-preview' # this might change in the future\n",
    "\n",
    "\n",
    "if \"AZURE_OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\n",
    "        \"Enter your AzureOpenAI API key: \"\n",
    "    )\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = api_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define document path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "        model=llm_deployment_name,\n",
    "        azure_deployment=llm_deployment_name,\n",
    "        api_version=api_version,\n",
    "         )\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to encode to both summary and chunk levels, sharing the page metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def encode_pdf_hierarchical(path, chunk_size=1000, chunk_overlap=200, is_string=False):\n",
    "    \"\"\"\n",
    "    Asynchronously encodes a PDF book into a hierarchical vector store using OpenAI embeddings.\n",
    "    Includes rate limit handling with exponential backoff.\n",
    "    \n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple containing two FAISS vector stores:\n",
    "        1. Document-level summaries\n",
    "        2. Detailed chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load PDF documents\n",
    "    if not is_string:\n",
    "        loader = PyPDFLoader(path)\n",
    "        documents = await asyncio.to_thread(loader.load)\n",
    "    else:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            # Set a really small chunk size, just to show.\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        documents = text_splitter.create_documents([path])\n",
    "\n",
    "\n",
    "    # Create document-level summaries\n",
    "    summary_llm = AzureChatOpenAI(\n",
    "        model=llm_deployment_name,\n",
    "        azure_deployment=llm_deployment_name,\n",
    "        api_version=api_version,\n",
    "        \n",
    "    )\n",
    "    # summary_llm = AzureChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "    summary_chain = load_summarize_chain(summary_llm, chain_type=\"map_reduce\")\n",
    "    \n",
    "    async def summarize_doc(doc):\n",
    "        \"\"\"\n",
    "        Summarizes a single document with rate limit handling.\n",
    "        \n",
    "        Args:\n",
    "            doc: The document to be summarized.\n",
    "            \n",
    "        Returns:\n",
    "            A summarized Document object.\n",
    "        \"\"\"\n",
    "        # Retry the summarization with exponential backoff\n",
    "        summary_output = await retry_with_exponential_backoff(summary_chain.ainvoke([doc]))\n",
    "        summary = summary_output['output_text']\n",
    "        return Document(\n",
    "            page_content=summary,\n",
    "            metadata={\"source\": path, \"page\": doc.metadata[\"page\"], \"summary\": True}\n",
    "        )\n",
    "\n",
    "    # Process documents in smaller batches to avoid rate limits\n",
    "    batch_size = 5  # Adjust this based on your rate limits\n",
    "    summaries = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        batch_summaries = await asyncio.gather(*[summarize_doc(doc) for doc in batch])\n",
    "        summaries.extend(batch_summaries)\n",
    "        await asyncio.sleep(1)  # Short pause between batches\n",
    "\n",
    "    # Split documents into detailed chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    detailed_chunks = await asyncio.to_thread(text_splitter.split_documents, documents)\n",
    "\n",
    "    # Update metadata for detailed chunks\n",
    "    for i, chunk in enumerate(detailed_chunks):\n",
    "        chunk.metadata.update({\n",
    "            \"chunk_id\": i,\n",
    "            \"summary\": False,\n",
    "            \"page\": int(chunk.metadata.get(\"page\", 0))\n",
    "        })\n",
    "\n",
    "    # Create embeddings\n",
    "    embeddings = AzureOpenAIEmbeddings()\n",
    "\n",
    "    # Create vector stores asynchronously with rate limit handling\n",
    "    async def create_vectorstore(docs):\n",
    "        \"\"\"\n",
    "        Creates a vector store from a list of documents with rate limit handling.\n",
    "        \n",
    "        Args:\n",
    "            docs: The list of documents to be embedded.\n",
    "            \n",
    "        Returns:\n",
    "            A FAISS vector store containing the embedded documents.\n",
    "        \"\"\"\n",
    "        return await retry_with_exponential_backoff(\n",
    "            asyncio.to_thread(FAISS.from_documents, docs, embeddings)\n",
    "        )\n",
    "\n",
    "    # Generate vector stores for summaries and detailed chunks concurrently\n",
    "    summary_vectorstore, detailed_vectorstore = await asyncio.gather(\n",
    "        create_vectorstore(summaries),\n",
    "        create_vectorstore(detailed_chunks)\n",
    "    )\n",
    "\n",
    "    return summary_vectorstore, detailed_vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the PDF book to both document-level summaries and detailed chunks if the vector stores do not exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../vector_stores/summary_store\") and os.path.exists(\"../vector_stores/detailed_store\"):\n",
    "   embeddings = AzureOpenAIEmbeddings()\n",
    "   summary_store = FAISS.load_local(\"../vector_stores/summary_store\", embeddings, allow_dangerous_deserialization=True)\n",
    "   detailed_store = FAISS.load_local(\"../vector_stores/detailed_store\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "else:\n",
    "    summary_store, detailed_store = await encode_pdf_hierarchical(path)\n",
    "    summary_store.save_local(\"../vector_stores/summary_store\")\n",
    "    detailed_store.save_local(\"../vector_stores/detailed_store\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve information according to summary level, and then retrieve information from the chunk level vector store and filter according to the summary level pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hierarchical(query, summary_vectorstore, detailed_vectorstore, k_summaries=3, k_chunks=5):\n",
    "    \"\"\"\n",
    "    Performs a hierarchical retrieval using the query.\n",
    "\n",
    "    Args:\n",
    "        query: The search query.\n",
    "        summary_vectorstore: The vector store containing document summaries.\n",
    "        detailed_vectorstore: The vector store containing detailed chunks.\n",
    "        k_summaries: The number of top summaries to retrieve.\n",
    "        k_chunks: The number of detailed chunks to retrieve per summary.\n",
    "\n",
    "    Returns:\n",
    "        A list of relevant detailed chunks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve top summaries\n",
    "    top_summaries = summary_vectorstore.similarity_search(query, k=k_summaries)\n",
    "    \n",
    "    relevant_chunks = []\n",
    "    for summary in top_summaries:\n",
    "        # For each summary, retrieve relevant detailed chunks\n",
    "        page_number = summary.metadata[\"page\"]\n",
    "        page_filter = lambda metadata: metadata[\"page\"] == page_number\n",
    "        page_chunks = detailed_vectorstore.similarity_search(\n",
    "            query, \n",
    "            k=k_chunks, \n",
    "            filter=page_filter\n",
    "        )\n",
    "        relevant_chunks.extend(page_chunks)\n",
    "    \n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate on a use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the greenhouse effect?\"\n",
    "results = retrieve_hierarchical(query, summary_store, detailed_store)\n",
    "\n",
    "# Print results\n",
    "for chunk in results:\n",
    "    print(f\"Page: {chunk.metadata['page']}\")\n",
    "    print(f\"Content: {chunk.page_content}...\")  # Print first 100 characters\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# Assume we already have the retrieve_hierarchical function and the vector stores initialized\n",
    "\n",
    "def query_with_hierarchical_retrieval(query, summary_vectorstore, detailed_vectorstore, llm):\n",
    "    # Step 1: Use the hierarchical retrieval function to get relevant chunks\n",
    "    relevant_chunks = retrieve_hierarchical(query, summary_vectorstore, detailed_vectorstore)\n",
    "    \n",
    "    # Combine the relevant chunks into a context string\n",
    "    context = \"\\n\\n\".join([chunk.page_content for chunk in relevant_chunks])\n",
    "    \n",
    "    # Step 2: Define a prompt for the LLM to use this context to generate an answer\n",
    "    prompt_template = \"\"\"\n",
    "    Given the following context, answer the query:\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Query:\n",
    "    {query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the prompt template\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"query\"])\n",
    "    \n",
    "    # Step 3: Run the query through the LLM\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    answer = chain.run(context=context, query=query)\n",
    "    \n",
    "    return answer, relevant_chunks\n",
    "\n",
    "\n",
    "query = \"What are the key benefits of sustainable energy solutions?\"\n",
    "answer, _ = query_with_hierarchical_retrieval(query, summary_store, detailed_store, llm)\n",
    "answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "## Metrics\n",
    "\n",
    "### Faithfulness\n",
    "\n",
    "This measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better.\n",
    "\n",
    "The generated answer is regarded as faithful if all the claims made in the answer can be inferred from the given context. To calculate this, a set of claims from the generated answer is first identified. Then each of these claims is cross-checked with the given context to determine if it can be inferred from the context.\n",
    "\n",
    "\n",
    "### Response Relevancy\n",
    "The evaluation metric, Answer Relevancy, focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy. This metric is computed using the user_input, the retrived_contexts and the response.\n",
    "\n",
    "\n",
    "### Context Recall\n",
    "Context Recall measures how many of the relevant documents (or pieces of information) were successfully retrieved. It focuses on not missing important results. Higher recall means fewer relevant documents were left out. In short, recall is about not missing anything important. Since it is about not missing anything, calculating context recall always requires a reference to compare against.\n",
    "\n",
    "### Context Precision\n",
    "Context Precision is a metric that measures the proportion of relevant chunks in the retrieved_contexts. It is calculated as the mean of the precision@k for each chunk in the context. Precision@k is the ratio of the number of relevant chunks at rank k to the total number of chunks at rank k.\n",
    "\n",
    "\n",
    "\n",
    "### Answer Correctness\n",
    "\n",
    "The assessment of Answer Correctness involves gauging the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness.\n",
    "\n",
    "Answer correctness encompasses two critical aspects: semantic similarity between the generated answer and the ground truth, as well as factual similarity. These aspects are combined using a weighted scheme to formulate the answer correctness score. Users also have the option to employ a 'threshold' value to round the resulting score to binary, if desired.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    answer_correctness,\n",
    ")\n",
    "\n",
    "# list of metrics we're going to use\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the test set from the specified path\n",
    "testset_path = \"../data/testset\"\n",
    "testset_df = load_from_disk(testset_path)\n",
    "testset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_responses(dataset, summary_store, detailed_store, llm):\n",
    "    \"\"\"\n",
    "    Populates the 'response' and 'contexts' columns in the dataset using the query_with_hierarchical_retrieval function.\n",
    "    Also renames the existing 'contexts' column to 'groundtruth_contexts'.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset containing the questions.\n",
    "        summary_store: The vector store containing document summaries.\n",
    "        detailed_store: The vector store containing detailed chunks.\n",
    "        llm: The language model to generate responses.\n",
    "\n",
    "    Returns:\n",
    "        The dataset with the 'response' and 'contexts' columns populated and 'contexts' renamed to 'groundtruth_contexts'.\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    retrieved_contexts = []\n",
    "    \n",
    "    for question in tqdm(dataset['question'], desc=\"Processing questions\"):\n",
    "        response, context_chunks = query_with_hierarchical_retrieval(question, summary_store, detailed_store, llm)\n",
    "        relevant_context = [chunk.page_content for chunk in context_chunks]\n",
    "        responses.append(response)\n",
    "        retrieved_contexts.append(relevant_context)\n",
    "    \n",
    "    # Rename the existing 'contexts' column to 'groundtruth_contexts'\n",
    "    dataset = dataset.rename_column('contexts', 'groundtruth_contexts')\n",
    "    \n",
    "    # Add the new 'response' and 'contexts' columns\n",
    "    dataset = dataset.add_column('response', responses)\n",
    "    dataset = dataset.add_column('contexts', retrieved_contexts)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Populate the 'response' and 'contexts' columns in the testset_df\n",
    "testset_df = populate_responses(testset_df, summary_store, detailed_store, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df[0]['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "result = evaluate(testset_df, metrics=metrics, llm=llm, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_pandas().iloc[0].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_pandas().iloc[0].contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
